{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import nltk\n",
    "from pylab import rcParams\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from transformers import BertTokenizer\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "import transformers\n",
    "from transformers import BertModel, BertTokenizer, AdamW, get_linear_schedule_with_warmup, BertForSequenceClassification\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from collections import defaultdict\n",
    "from textwrap import wrap\n",
    "\n",
    "from datasets import load_dataset\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from transformers import AdamW, get_scheduler\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=3)\n",
    "\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "sia = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "am1 = pd.read_csv('AM1_filtered_data.csv')\n",
    "am1 = am1[['Type', 'Situation']]\n",
    "am2 = pd.read_csv('AM2.csv') #has all rows\n",
    "am2 = am2[['Type', 'Situation']]\n",
    "am3 = pd.read_csv('AM3.csv') #has only headline\n",
    "am3 = am3.rename(columns={'Headline': 'Situation'})\n",
    "am3['Type'] = pd.NA\n",
    "am3 = am3[['Type', 'Situation']]\n",
    "am4 = pd.read_csv('AM4.csv') #has only headline\n",
    "am4 = am4.rename(columns={'Headline': 'Situation'})\n",
    "am4['Type'] = pd.NA\n",
    "am4 = am4[['Type', 'Situation']]\n",
    "\n",
    "zaki = pd.read_csv('zaki_filtered_data.csv')\n",
    "zaki = zaki[['Type', 'Situation']]\n",
    "\n",
    "sean = pd.read_csv('sean.csv')\n",
    "sean = sean[['type', 'situations']]\n",
    "sean = sean.rename(columns={'situations': 'Situation', 'type': 'Type'})\n",
    "\n",
    "james1 = pd.read_csv('james_3k_filtered_data.csv')\n",
    "james2 = pd.read_csv('james_6k_filtered_data.csv')\n",
    "james1 = james1.rename(columns={'situation': 'Situation', 'type': 'Type'})\n",
    "james2 = james2.rename(columns={'situation': 'Situation', 'type': 'Type'})\n",
    "\n",
    "all_data = pd.concat([am1, am2, am3, am4, zaki, james1, james2, sean], ignore_index=True)\n",
    "\n",
    "capitaliq_train, capitaliq_val = train_test_split(all_data, test_size=0.2, random_state=42)\n",
    "\n",
    "capitaliq_train = capitaliq_train.reset_index(drop=True)\n",
    "capitaliq_val = capitaliq_val.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentiment(text):\n",
    "    if isinstance(text, str):  # Ensure the input is a string\n",
    "        score = sia.polarity_scores(text)['compound']  # Get the compound score from VADER\n",
    "        if score >= 0.05:\n",
    "            return 1  # Positive sentiment\n",
    "        elif score <= -0.05:\n",
    "            return -1  # Negative sentiment\n",
    "        else:\n",
    "            return 0  # Neutral sentiment\n",
    "    else:\n",
    "        return np.nan  # Return NaN for non-string inputs\n",
    "\n",
    "capitaliq_train['sentiment'] = capitaliq_train['Situation'].apply(get_sentiment)\n",
    "capitaliq_val['sentiment'] = capitaliq_val['Situation'].apply(get_sentiment)\n",
    "\n",
    "capitaliq_train['sentiment'] = capitaliq_train['sentiment'].replace({-1: 0, 0: 1, 1: 2})\n",
    "capitaliq_val['sentiment'] = capitaliq_val['sentiment'].replace({-1: 0, 0: 1, 1: 2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "external_data_train = pd.read_csv('external_data_train.csv')\n",
    "external_data_val = pd.read_csv('external_data_test.csv')\n",
    "\n",
    "external_data_val = external_data_val.rename(columns={'text': 'Situation'})\n",
    "external_data_train = external_data_train.rename(columns={'text': 'Situation'})\n",
    "\n",
    "train = pd.concat([external_data_train, capitaliq_train], ignore_index=True)\n",
    "val = pd.concat([external_data_val, capitaliq_val], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "types_to_remove = [\n",
    "    'Fixed Income Offering',\n",
    "    'Follow-on Equity Offering',\n",
    "    'Annual General Meeting',\n",
    "    'Amazon Web Services, Inc.',\n",
    "    'Amazon.com, Inc. (NasdaqGS:AMZN)',\n",
    "    'Amazon.com Inc., Investment Arm',\n",
    "    'Amazon Connect Technology Services (Beijing) Co., Ltd.',\n",
    "    'Amazon.Com NV Investment Holdings, LLC',\n",
    "    'Amazon Web Services India Private Limited',\n",
    "    'Amazon Logistics Inc.',\n",
    "    'Amazon Web Services, Inc. (NasdaqGS:AMWY)',\n",
    "    'End of Lock-Up Period',\n",
    "    'Amazon Web Services, Inc. (NasdaqGS:AMZN)',\n",
    "    'Amazon.com Services LLC' ,\n",
    "    'Public Offering Lead Underwriter Change',\n",
    "    'Shelf Registration Filing',\n",
    "    'Company Conference Presentation',\n",
    "    'Conference',\n",
    "    'Zappos.com LLC',\n",
    "    'Amazon India Ltd.',\n",
    "    'Ticker Change',\n",
    "    'Imdb.Com, Inc.',\n",
    "    'Board Meeting',\n",
    "    'Ex-Div Date (Regular)',\n",
    "    'Index Constituent Add',\n",
    "    'Index Constituent Drop',\n",
    "    'Address Change',\n",
    "    'Whole Foods Market, Inc.',\n",
    "    '1Life Healthcare, Inc.',\n",
    "    'Dividend Affirmation',\n",
    "    'Earnings Release Date',\n",
    "    'Amazon Web Services India Private Limited',\n",
    "    'Amazon Digital Services LLC',\n",
    "    'Deliveroo plc (LSE:ROO)',\n",
    "    'TWSE:2330',\n",
    "    'TWSE:6789',\n",
    "    'NeuralGarage Pvt Ltd',\n",
    "    'Amazon Robotics LLC',\n",
    "    'Mgm Interactive Inc.',\n",
    "    'Zoox Inc.',\n",
    "    'Alchip Technologies, Limited (TWSE:3661)',\n",
    "    'Special/Extraordinary Shareholders Meeting',\n",
    "    'Ex-Div Date (Special)',\n",
    "    'Dividend Decrease',\n",
    "    'Delayed SEC Filing'\n",
    "]\n",
    "\n",
    "train_final = train[~train['Type'].isin(types_to_remove)]\n",
    "val_final = val[~val['Type'].isin(types_to_remove)]\n",
    "\n",
    "train = train.dropna(subset=['Situation'])\n",
    "val = val.dropna(subset=['Situation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Tokenize the text data for both train and validation sets\n",
    "train_encodings = tokenizer(train['Situation'].tolist(), truncation=True, padding='max_length', max_length=128)\n",
    "val_encodings = tokenizer(val['Situation'].tolist(), truncation=True, padding='max_length', max_length=128)\n",
    "\n",
    "# Convert sentiment labels into tensors\n",
    "train_labels = torch.tensor(train['sentiment'].tolist(), dtype=torch.long)\n",
    "val_labels = torch.tensor(val['sentiment'].tolist(), dtype=torch.long)\n",
    "\n",
    "# Convert the tokenized inputs into PyTorch tensors\n",
    "train_input_ids = torch.tensor(train_encodings['input_ids'])\n",
    "train_attention_mask = torch.tensor(train_encodings['attention_mask'])\n",
    "\n",
    "val_input_ids = torch.tensor(val_encodings['input_ids'])\n",
    "val_attention_mask = torch.tensor(val_encodings['attention_mask'])\n",
    "\n",
    "# Create TensorDatasets for train and validation sets\n",
    "train_dataset = TensorDataset(train_input_ids, train_attention_mask, train_labels)\n",
    "val_dataset = TensorDataset(val_input_ids, val_attention_mask, val_labels)\n",
    "\n",
    "# Create DataLoaders for train and validation sets\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = GradScaler()\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "# Move the model to the device\n",
    "model = model.to(device)\n",
    "\n",
    "# Function for performing the validation loop\n",
    "def validate(model, val_dataloader, device):\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "\n",
    "    # Validation progress bar\n",
    "    val_progress = tqdm(val_dataloader, desc=\"Validating\", leave=False)\n",
    "\n",
    "    for batch in val_progress:\n",
    "        # Unpack the batch (input_ids, attention_mask, labels) and move to device\n",
    "        input_ids, attention_mask, labels = [t.to(device) for t in batch]  # Move to correct device\n",
    "\n",
    "        # Cast labels to Long for CrossEntropyLoss\n",
    "        labels = labels.long()\n",
    "\n",
    "        # No gradient calculation for validation\n",
    "        with torch.no_grad():\n",
    "            # Forward pass\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            logits = outputs.logits\n",
    "\n",
    "        # Get predicted class (highest logit value)\n",
    "        predictions = torch.argmax(logits, dim=-1)\n",
    "\n",
    "        # Move tensors back to CPU for metric calculation\n",
    "        all_predictions.extend(predictions.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    # Calculate accuracy and F1 score\n",
    "    accuracy = accuracy_score(all_labels, all_predictions)\n",
    "    f1 = f1_score(all_labels, all_predictions, average='weighted')  # Calculate weighted F1 score\n",
    "    return accuracy, f1\n",
    "\n",
    "# Define optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5, weight_decay=0.01)\n",
    "\n",
    "# Set up learning rate scheduler\n",
    "num_epochs = 8\n",
    "num_training_steps = num_epochs * len(train_dataloader)\n",
    "lr_scheduler = get_scheduler(\n",
    "    name=\"linear\", optimizer=optimizer, num_warmup_steps=0.1 * num_training_steps, num_training_steps=num_training_steps\n",
    ")\n",
    "\n",
    "# Training loop with progress bar for each epoch\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "\n",
    "    # Set the model to training mode\n",
    "    model.train()\n",
    "\n",
    "    # Create a progress bar for the current epoch\n",
    "    epoch_progress = tqdm(train_dataloader, desc=f\"Training Epoch {epoch + 1}\", leave=False)\n",
    "\n",
    "    for batch in epoch_progress:\n",
    "        # Unpack the batch\n",
    "        input_ids, attention_mask, labels = batch\n",
    "\n",
    "        # Move to device\n",
    "        input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device).long()\n",
    "\n",
    "        # Forward pass with mixed-precision (updated autocast syntax)\n",
    "        with torch.amp.autocast(device_type='cuda', enabled=torch.cuda.is_available()):\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            logits = outputs.logits\n",
    "            loss_fn = torch.nn.CrossEntropyLoss()\n",
    "            loss = loss_fn(logits, labels)\n",
    "\n",
    "        # Backward pass with scaled gradients\n",
    "        scaler.scale(loss).backward()\n",
    "\n",
    "        # Unscale gradients and perform optimizer step\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        # Step learning rate scheduler\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Optionally, update the progress bar with the current loss\n",
    "        epoch_progress.set_postfix({'loss': loss.item()})\n",
    "\n",
    "    # Run validation after each epoch and compute accuracy and F1 score\n",
    "    accuracy, f1 = validate(model, val_dataloader, device)\n",
    "    print(f\"Validation Accuracy after epoch {epoch + 1}: {accuracy}\")\n",
    "    print(f\"Validation F1 Score after epoch {epoch + 1}: {f1}\")\n",
    "\n",
    "# Final evaluation on the validation set\n",
    "final_accuracy, final_f1 = validate(model, val_dataloader, device)\n",
    "print(f\"Final Validation Accuracy: {final_accuracy}\")\n",
    "print(f\"Final Validation F1 Score: {final_f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'NLP_Project_Model92.2.pth')\n",
    "\n",
    "# !zip -r trained_model.zip trained_model/\n",
    "# from google.colab import files\n",
    "# files.download('NLP_Project_Model92.2.zip')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4c984aae39f9b63f29ed0fb7cd34d780f5f68de00193e63aeb21699a2279943c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
